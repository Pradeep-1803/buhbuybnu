{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sleep Disorder Classification v2 — Research-Grade Pipeline\n",
                "\n",
                "**Key fixes from v1**: Deduplication, class weights (no SMOTE), Stratified 5-Fold CV, improved architectures, XGBoost added.\n",
                "\n",
                "> Set runtime to **T4 GPU**: Runtime → Change runtime type"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup\n",
                "!pip install -q optuna xgboost seaborn\n",
                "\n",
                "import torch, os, glob, joblib, warnings\n",
                "import numpy as np, pandas as pd\n",
                "import matplotlib.pyplot as plt, seaborn as sns\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
                "                             recall_score, confusion_matrix, classification_report)\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "from xgboost import XGBClassifier\n",
                "import torch.nn as nn, torch.optim as optim\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "USE_AMP = torch.cuda.is_available()\n",
                "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
                "print(f'Device: {DEVICE}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Mount Drive & Upload CSV\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "SAVE_DIR = '/content/drive/MyDrive/Sleep_Disorder_Project'\n",
                "CKPT_DIR = os.path.join(SAVE_DIR, 'checkpoints_v2')\n",
                "os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "\n",
                "csv_path = 'sleep_dataset.csv'\n",
                "if not os.path.exists(csv_path):\n",
                "    # Check Drive first\n",
                "    drive_csv = os.path.join(SAVE_DIR, 'sleep_dataset.csv')\n",
                "    if os.path.exists(drive_csv):\n",
                "        csv_path = drive_csv\n",
                "    else:\n",
                "        from google.colab import files\n",
                "        print('Upload sleep_dataset.csv:')\n",
                "        uploaded = files.upload()\n",
                "        for fn in uploaded: csv_path = fn\n",
                "\n",
                "print(f'Using CSV: {csv_path}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load, DEDUPLICATE, & Preprocess\n",
                "df = pd.read_csv(csv_path)\n",
                "print(f'Raw dataset: {len(df)} rows')\n",
                "print(f'Columns: {list(df.columns)}')\n",
                "\n",
                "# CRITICAL FIX: Remove duplicates\n",
                "if 'Person ID' in df.columns:\n",
                "    df = df.drop(columns=['Person ID'])\n",
                "df = df.drop_duplicates()\n",
                "print(f'After deduplication: {len(df)} unique rows')\n",
                "\n",
                "# Handle target\n",
                "df['Sleep Disorder'] = df['Sleep Disorder'].fillna('None')\n",
                "print(f'\\nClass distribution:')\n",
                "print(df['Sleep Disorder'].value_counts())\n",
                "\n",
                "# Feature engineering\n",
                "if 'Blood Pressure' in df.columns:\n",
                "    df[['Systolic_BP', 'Diastolic_BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(float)\n",
                "    df = df.drop(columns=['Blood Pressure'])\n",
                "if 'BMI Category' in df.columns:\n",
                "    df['BMI Category'] = df['BMI Category'].replace({'Normal Weight': 'Normal'})\n",
                "\n",
                "# Encode target\n",
                "le = LabelEncoder()\n",
                "target = 'Sleep Disorder'\n",
                "y = le.fit_transform(df[target])\n",
                "print(f'\\nEncoded classes: {dict(zip(le.classes_, range(len(le.classes_))))}')\n",
                "\n",
                "# Preprocess features\n",
                "cat_cols = ['Gender', 'Occupation', 'BMI Category']\n",
                "num_cols = [c for c in df.columns if c not in cat_cols + [target]]\n",
                "preprocessor = ColumnTransformer([\n",
                "    ('num', Pipeline([('scaler', StandardScaler())]), num_cols),\n",
                "    ('cat', Pipeline([('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), cat_cols)\n",
                "])\n",
                "X = preprocessor.fit_transform(df.drop(columns=[target]))\n",
                "print(f'Feature matrix: {X.shape}')\n",
                "\n",
                "# Save label encoder\n",
                "joblib.dump(le, os.path.join(SAVE_DIR, 'label_encoder_v2.joblib'))\n",
                "print('Done!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Utility Functions\n",
                "\n",
                "class SleepDataset(Dataset):\n",
                "    def __init__(self, X, y):\n",
                "        self.X = torch.tensor(X, dtype=torch.float32)\n",
                "        self.y = torch.tensor(y, dtype=torch.long)\n",
                "    def __len__(self): return len(self.X)\n",
                "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
                "\n",
                "def plot_cm(model_name, y_true, y_pred, class_names, fold, split, ax):\n",
                "    \"\"\"Plot confusion matrix on given axis.\"\"\"\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    acc = accuracy_score(y_true, y_pred)\n",
                "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "                xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
                "    ax.set_title(f'Fold {fold+1} {split}\\nAcc:{acc:.3f} F1:{f1:.3f}', fontsize=10)\n",
                "    ax.set_ylabel('True'); ax.set_xlabel('Pred')\n",
                "    return acc, f1\n",
                "\n",
                "def get_ckpt_ver(name, d):\n",
                "    files = glob.glob(os.path.join(d, f'{name}_v*.*'))\n",
                "    nums = []\n",
                "    for f in files:\n",
                "        try: nums.append(int(os.path.basename(f).split('_v')[1].split('.')[0]))\n",
                "        except: pass\n",
                "    return max(nums)+1 if nums else 1\n",
                "\n",
                "def save_ckpt(model, name, ckpt_dir, pytorch=False, meta=None):\n",
                "    ver = get_ckpt_ver(name, ckpt_dir)\n",
                "    if pytorch:\n",
                "        path = os.path.join(ckpt_dir, f'{name}_v{ver}.pth')\n",
                "        torch.save({'state_dict': model.state_dict(), 'v': ver, 'meta': meta or {}}, path)\n",
                "    else:\n",
                "        path = os.path.join(ckpt_dir, f'{name}_v{ver}.joblib')\n",
                "        joblib.dump({'model': model, 'v': ver, 'meta': meta or {}}, path)\n",
                "    print(f'  Checkpoint: {os.path.basename(path)}')\n",
                "    return path\n",
                "\n",
                "def compute_weights(y_train):\n",
                "    \"\"\"Compute class weights for imbalanced data.\"\"\"\n",
                "    cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
                "    return cw\n",
                "\n",
                "print('Utilities loaded.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Model Definitions\n",
                "\n",
                "class ANN(nn.Module):\n",
                "    \"\"\"3-layer MLP with BatchNorm, LeakyReLU, Dropout.\"\"\"\n",
                "    def __init__(self, dim, nc):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(dim, 128), nn.BatchNorm1d(128), nn.LeakyReLU(0.1), nn.Dropout(0.3),\n",
                "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.LeakyReLU(0.1), nn.Dropout(0.3),\n",
                "            nn.Linear(64, 32), nn.BatchNorm1d(32), nn.LeakyReLU(0.1), nn.Dropout(0.2),\n",
                "            nn.Linear(32, nc)\n",
                "        )\n",
                "    def forward(self, x): return self.net(x)\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    \"\"\"2-layer 1D CNN with GlobalAvgPool.\"\"\"\n",
                "    def __init__(self, dim, nc):\n",
                "        super().__init__()\n",
                "        self.features = nn.Sequential(\n",
                "            nn.Conv1d(1, 32, kernel_size=3, padding=1), nn.BatchNorm1d(32), nn.ReLU(),\n",
                "            nn.Conv1d(32, 64, kernel_size=3, padding=1), nn.BatchNorm1d(64), nn.ReLU(),\n",
                "            nn.AdaptiveAvgPool1d(1)\n",
                "        )\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(64, nc)\n",
                "        )\n",
                "    def forward(self, x):\n",
                "        x = self.features(x.unsqueeze(1)).squeeze(-1)\n",
                "        return self.classifier(x)\n",
                "\n",
                "def train_torch(model, X_tr, y_tr, X_te, y_te, class_weights, epochs=100, lr=0.001):\n",
                "    \"\"\"Train PyTorch model with class-weighted loss, AMP, and LR scheduling.\"\"\"\n",
                "    model.to(DEVICE)\n",
                "    w = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
                "    criterion = nn.CrossEntropyLoss(weight=w)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
                "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
                "    scaler = GradScaler(enabled=USE_AMP)\n",
                "\n",
                "    train_dl = DataLoader(SleepDataset(X_tr, y_tr), batch_size=BATCH_SIZE,\n",
                "                          shuffle=True, pin_memory=USE_AMP)\n",
                "    best_f1, best_state = 0, None\n",
                "\n",
                "    for ep in range(epochs):\n",
                "        model.train()\n",
                "        for Xb, yb in train_dl:\n",
                "            Xb, yb = Xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
                "            optimizer.zero_grad(set_to_none=True)\n",
                "            with autocast(enabled=USE_AMP):\n",
                "                loss = criterion(model(Xb), yb)\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "\n",
                "        # Validate\n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            Xt = torch.tensor(X_te, dtype=torch.float32).to(DEVICE)\n",
                "            preds = model(Xt).argmax(1).cpu().numpy()\n",
                "        vf1 = f1_score(y_te, preds, average='weighted')\n",
                "        scheduler.step(1 - vf1)\n",
                "\n",
                "        if vf1 > best_f1:\n",
                "            best_f1 = vf1\n",
                "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
                "\n",
                "        if (ep+1) % 25 == 0:\n",
                "            print(f'    Ep {ep+1}/{epochs} | F1: {vf1:.4f} | Best: {best_f1:.4f}')\n",
                "\n",
                "    model.load_state_dict(best_state)\n",
                "    return model\n",
                "\n",
                "def predict_torch(model, X):\n",
                "    model.eval()\n",
                "    with torch.no_grad(), autocast(enabled=USE_AMP):\n",
                "        Xt = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
                "        return model(Xt).argmax(1).cpu().numpy()\n",
                "\n",
                "print('Models defined.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Stratified 5-Fold CV Training Pipeline\n",
                "\n",
                "N_FOLDS = 5\n",
                "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
                "input_dim = X.shape[1]\n",
                "n_classes = len(le.classes_)\n",
                "\n",
                "# Define all models\n",
                "def get_models(cw_dict, cw_array):\n",
                "    \"\"\"Return dict of model name -> (model, is_pytorch).\"\"\"\n",
                "    # XGBoost sample weights will be handled separately\n",
                "    return {\n",
                "        'KNN': (KNeighborsClassifier(n_neighbors=5, weights='distance', metric='minkowski', n_jobs=-1), False),\n",
                "        'SVM': (SVC(C=10.0, kernel='rbf', gamma='scale', class_weight='balanced'), False),\n",
                "        'RF':  (RandomForestClassifier(n_estimators=300, max_depth=15, min_samples_split=5,\n",
                "                                       class_weight='balanced', random_state=42, n_jobs=-1), False),\n",
                "        'XGBoost': (XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.1,\n",
                "                                   subsample=0.8, colsample_bytree=0.8,\n",
                "                                   scale_pos_weight=1, eval_metric='mlogloss',\n",
                "                                   random_state=42, n_jobs=-1), False),\n",
                "        'ANN': (ANN(input_dim, n_classes), True),\n",
                "        'CNN': (CNN(input_dim, n_classes), True),\n",
                "    }\n",
                "\n",
                "print(f'Running {N_FOLDS}-Fold CV on {len(X)} samples, {input_dim} features, {n_classes} classes')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. TRAIN ALL MODELS\n",
                "\n",
                "model_names = ['KNN', 'SVM', 'RF', 'XGBoost', 'ANN', 'CNN']\n",
                "all_results = {m: {'acc': [], 'f1': [], 'prec': [], 'rec': []} for m in model_names}\n",
                "best_models = {}\n",
                "best_f1s = {m: 0 for m in model_names}\n",
                "\n",
                "for m_name in model_names:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"  {m_name} — Stratified {N_FOLDS}-Fold CV\")\n",
                "    print(f\"{'='*60}\")\n",
                "\n",
                "    fig, axes = plt.subplots(2, N_FOLDS, figsize=(4*N_FOLDS, 8))\n",
                "    fig.suptitle(f'{m_name} — Per-Fold Confusion Matrices', fontsize=14, fontweight='bold')\n",
                "\n",
                "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
                "        X_tr, X_te = X[train_idx], X[test_idx]\n",
                "        y_tr, y_te = y[train_idx], y[test_idx]\n",
                "\n",
                "        cw = compute_weights(y_tr)\n",
                "        cw_dict = dict(zip(np.unique(y_tr), cw))\n",
                "\n",
                "        # Create fresh model for each fold\n",
                "        models = get_models(cw_dict, cw)\n",
                "        model, is_pytorch = models[m_name]\n",
                "\n",
                "        print(f'  Fold {fold+1}/{N_FOLDS} (train={len(y_tr)}, test={len(y_te)})')\n",
                "\n",
                "        if is_pytorch:\n",
                "            # Deep copy for fresh weights\n",
                "            import copy\n",
                "            model = copy.deepcopy(models[m_name][0])\n",
                "            model = train_torch(model, X_tr, y_tr, X_te, y_te, cw, epochs=100, lr=0.001)\n",
                "            train_preds = predict_torch(model, X_tr)\n",
                "            test_preds = predict_torch(model, X_te)\n",
                "        else:\n",
                "            if m_name == 'XGBoost':\n",
                "                # XGBoost uses sample_weight\n",
                "                sw = np.array([cw_dict[yi] for yi in y_tr])\n",
                "                model.fit(X_tr, y_tr, sample_weight=sw)\n",
                "            else:\n",
                "                model.fit(X_tr, y_tr)\n",
                "            train_preds = model.predict(X_tr)\n",
                "            test_preds = model.predict(X_te)\n",
                "\n",
                "        # Metrics\n",
                "        acc = accuracy_score(y_te, test_preds)\n",
                "        f1 = f1_score(y_te, test_preds, average='weighted')\n",
                "        prec = precision_score(y_te, test_preds, average='weighted')\n",
                "        rec = recall_score(y_te, test_preds, average='weighted')\n",
                "        all_results[m_name]['acc'].append(acc)\n",
                "        all_results[m_name]['f1'].append(f1)\n",
                "        all_results[m_name]['prec'].append(prec)\n",
                "        all_results[m_name]['rec'].append(rec)\n",
                "\n",
                "        print(f'    Test Acc: {acc:.4f} | F1: {f1:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f}')\n",
                "\n",
                "        # Plot confusion matrices\n",
                "        plot_cm(m_name, y_tr, train_preds, le.classes_, fold, 'Train', axes[0, fold])\n",
                "        plot_cm(m_name, y_te, test_preds, le.classes_, fold, 'Test', axes[1, fold])\n",
                "\n",
                "        # Save best model\n",
                "        if f1 > best_f1s[m_name]:\n",
                "            best_f1s[m_name] = f1\n",
                "            if is_pytorch:\n",
                "                best_models[m_name] = copy.deepcopy(model)\n",
                "            else:\n",
                "                import copy as cp\n",
                "                best_models[m_name] = cp.deepcopy(model)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    cm_path = os.path.join(CKPT_DIR, f'{m_name}_confusion_v2.png')\n",
                "    plt.savefig(cm_path, dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(f'  Saved: {cm_path}')\n",
                "\n",
                "    # Save best model as checkpoint\n",
                "    best = best_models[m_name]\n",
                "    meta = {k: f'{np.mean(v):.4f}±{np.std(v):.4f}' for k, v in all_results[m_name].items()}\n",
                "    save_ckpt(best, m_name, CKPT_DIR, pytorch=(m_name in ['ANN', 'CNN']), meta=meta)\n",
                "\n",
                "    # Classification report for last fold\n",
                "    print(f'\\n  {m_name} Last Fold Report:')\n",
                "    print(classification_report(y_te, test_preds, target_names=le.classes_))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Final Summary\n",
                "print('\\n' + '='*70)\n",
                "print('  FINAL RESULTS — Stratified 5-Fold CV  (mean ± std)')\n",
                "print('='*70)\n",
                "\n",
                "summary = []\n",
                "for m in model_names:\n",
                "    r = all_results[m]\n",
                "    summary.append({\n",
                "        'Model': m,\n",
                "        'Accuracy': f\"{np.mean(r['acc']):.4f} ± {np.std(r['acc']):.4f}\",\n",
                "        'Precision': f\"{np.mean(r['prec']):.4f} ± {np.std(r['prec']):.4f}\",\n",
                "        'Recall': f\"{np.mean(r['rec']):.4f} ± {np.std(r['rec']):.4f}\",\n",
                "        'F1-Score': f\"{np.mean(r['f1']):.4f} ± {np.std(r['f1']):.4f}\",\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(summary).set_index('Model')\n",
                "print(results_df.to_string())\n",
                "\n",
                "# Paper comparison\n",
                "print('\\n--- Paper Baselines (GA-optimized) ---')\n",
                "paper = {'KNN': 83.19, 'SVM': 92.04, 'RF': 91.15, 'ANN': 92.92}\n",
                "for m, baseline in paper.items():\n",
                "    our = np.mean(all_results[m]['acc']) * 100\n",
                "    diff = our - baseline\n",
                "    status = '✅ BEAT' if diff > 0 else '❌ BELOW'\n",
                "    print(f'  {m:8s}: Paper={baseline:.2f}%  Ours={our:.2f}%  ({diff:+.2f}%)  [{status}]')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Feature Importance (Random Forest)\n",
                "if 'RF' in best_models:\n",
                "    rf_model = best_models['RF']\n",
                "    feat_names = (num_cols +\n",
                "                  list(preprocessor.named_transformers_['cat']\n",
                "                       .named_steps['ohe'].get_feature_names_out(cat_cols)))\n",
                "    importances = rf_model.feature_importances_\n",
                "    idx = np.argsort(importances)[::-1][:15]\n",
                "\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.bar(range(len(idx)), importances[idx])\n",
                "    plt.xticks(range(len(idx)), [feat_names[i] for i in idx], rotation=45, ha='right')\n",
                "    plt.title('Top 15 Feature Importances (Random Forest)')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(CKPT_DIR, 'feature_importance.png'), dpi=150)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. List Checkpoints\n",
                "print('\\nSaved checkpoints:')\n",
                "for f in sorted(os.listdir(CKPT_DIR)):\n",
                "    sz = os.path.getsize(os.path.join(CKPT_DIR, f)) / 1024\n",
                "    print(f'  {f} ({sz:.1f} KB)')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}