{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sleep Disorder Classification — Training with Checkpoints & Confusion Matrices\n",
                "\n",
                "**Loads preprocessed `.npy` files from Google Drive, trains 5 models with full diagnostics.**\n",
                "\n",
                "Features:\n",
                "- Loads saved `X_train_full.npy`, `X_test.npy`, etc. (no re-preprocessing)\n",
                "- Generates **separate confusion matrices** for Train / Validation / Test per model\n",
                "- Saves model weights as **numbered checkpoints** for iterative improvement\n",
                "\n",
                "> **Runtime**: Set to **T4 GPU** via Runtime → Change runtime type"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup\n",
                "!pip install -q optuna imbalanced-learn seaborn\n",
                "\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    print(f'GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB)')\n",
                "else:\n",
                "    print('WARNING: No GPU! Go to Runtime -> Change runtime type -> T4 GPU')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Mount Drive & Load Preprocessed Data\n",
                "from google.colab import drive\n",
                "import numpy as np\n",
                "import joblib\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "DATA_DIR = '/content/drive/MyDrive/Sleep_Disorder_Project'\n",
                "CKPT_DIR = os.path.join(DATA_DIR, 'checkpoints')\n",
                "os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "\n",
                "X_train_full = np.load(os.path.join(DATA_DIR, 'X_train_full.npy'))\n",
                "y_train_full = np.load(os.path.join(DATA_DIR, 'y_train_full.npy'))\n",
                "X_test = np.load(os.path.join(DATA_DIR, 'X_test.npy'))\n",
                "y_test = np.load(os.path.join(DATA_DIR, 'y_test.npy'))\n",
                "le = joblib.load(os.path.join(DATA_DIR, 'label_encoder.joblib'))\n",
                "\n",
                "print(f'Train: {X_train_full.shape}, Test: {X_test.shape}')\n",
                "print(f'Classes: {list(le.classes_)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Create Train/Val Split\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full\n",
                ")\n",
                "print(f'Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Confusion Matrix Plotting Utility\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
                "\n",
                "def plot_confusion_matrices(model_name, y_sets, pred_sets, class_names, save_dir=None):\n",
                "    \"\"\"\n",
                "    Plot side-by-side confusion matrices for train/val/test.\n",
                "    y_sets: dict of {'Train': y_train, 'Val': y_val, 'Test': y_test}\n",
                "    pred_sets: dict of {'Train': preds_train, 'Val': preds_val, 'Test': preds_test}\n",
                "    \"\"\"\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
                "    fig.suptitle(f'{model_name} — Confusion Matrices', fontsize=16, fontweight='bold')\n",
                "\n",
                "    for ax, split_name in zip(axes, ['Train', 'Val', 'Test']):\n",
                "        y_true = y_sets[split_name]\n",
                "        y_pred = pred_sets[split_name]\n",
                "        cm = confusion_matrix(y_true, y_pred)\n",
                "        acc = accuracy_score(y_true, y_pred)\n",
                "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
                "\n",
                "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names,\n",
                "                    yticklabels=class_names, ax=ax)\n",
                "        ax.set_title(f'{split_name}\\nAcc: {acc:.4f} | F1: {f1:.4f}')\n",
                "        ax.set_ylabel('True')\n",
                "        ax.set_xlabel('Predicted')\n",
                "\n",
                "    plt.tight_layout()\n",
                "    if save_dir:\n",
                "        path = os.path.join(save_dir, f'{model_name}_confusion_matrices.png')\n",
                "        plt.savefig(path, dpi=150, bbox_inches='tight')\n",
                "        print(f'Saved: {path}')\n",
                "    plt.show()\n",
                "\n",
                "    # Print classification reports\n",
                "    for split_name in ['Train', 'Val', 'Test']:\n",
                "        print(f\"\\n--- {model_name} {split_name} Classification Report ---\")\n",
                "        print(classification_report(y_sets[split_name], pred_sets[split_name], target_names=class_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Checkpoint Utilities\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "import glob\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "USE_AMP = torch.cuda.is_available()\n",
                "BATCH_SIZE = 512 if torch.cuda.is_available() else 64\n",
                "\n",
                "class SleepDataset(Dataset):\n",
                "    def __init__(self, X, y):\n",
                "        self.X = torch.tensor(X, dtype=torch.float32)\n",
                "        self.y = torch.tensor(y, dtype=torch.long)\n",
                "    def __len__(self): return len(self.X)\n",
                "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
                "\n",
                "\n",
                "def get_next_checkpoint_num(model_name, ckpt_dir):\n",
                "    \"\"\"Find the next checkpoint number for a model.\"\"\"\n",
                "    existing = glob.glob(os.path.join(ckpt_dir, f'{model_name}_v*.pth')) + \\\n",
                "               glob.glob(os.path.join(ckpt_dir, f'{model_name}_v*.joblib'))\n",
                "    if not existing:\n",
                "        return 1\n",
                "    nums = []\n",
                "    for f in existing:\n",
                "        base = os.path.basename(f)\n",
                "        try:\n",
                "            num = int(base.split('_v')[1].split('.')[0])\n",
                "            nums.append(num)\n",
                "        except:\n",
                "            pass\n",
                "    return max(nums) + 1 if nums else 1\n",
                "\n",
                "\n",
                "def save_checkpoint(model, model_name, ckpt_dir, is_pytorch=False, metrics=None):\n",
                "    \"\"\"Save model with versioned checkpoint name.\"\"\"\n",
                "    ver = get_next_checkpoint_num(model_name, ckpt_dir)\n",
                "    if is_pytorch:\n",
                "        path = os.path.join(ckpt_dir, f'{model_name}_v{ver}.pth')\n",
                "        checkpoint = {\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'version': ver,\n",
                "            'metrics': metrics or {}\n",
                "        }\n",
                "        torch.save(checkpoint, path)\n",
                "    else:\n",
                "        path = os.path.join(ckpt_dir, f'{model_name}_v{ver}.joblib')\n",
                "        joblib.dump({'model': model, 'version': ver, 'metrics': metrics or {}}, path)\n",
                "    print(f'Checkpoint saved: {path} (v{ver})')\n",
                "    return path\n",
                "\n",
                "\n",
                "def make_loaders(*args_pairs, batch_size=BATCH_SIZE):\n",
                "    \"\"\"Create DataLoaders from (X, y) pairs.\"\"\"\n",
                "    pin = torch.cuda.is_available()\n",
                "    loaders = []\n",
                "    for X, y in args_pairs:\n",
                "        ds = SleepDataset(X, y)\n",
                "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, pin_memory=pin, num_workers=2)\n",
                "        loaders.append(dl)\n",
                "    return loaders\n",
                "\n",
                "\n",
                "def predict_torch(model, loader):\n",
                "    \"\"\"Get predictions from a PyTorch model.\"\"\"\n",
                "    model.eval()\n",
                "    preds = []\n",
                "    with torch.no_grad(), autocast(enabled=USE_AMP):\n",
                "        for Xb, _ in loader:\n",
                "            out = model(Xb.to(DEVICE, non_blocking=True))\n",
                "            preds.extend(torch.max(out, 1)[1].cpu().numpy())\n",
                "    return np.array(preds)\n",
                "\n",
                "print(f'Device: {DEVICE} | Batch: {BATCH_SIZE} | AMP: {USE_AMP}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Model Definitions\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "class ANN(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_layers=2, units=128, dropout=0.3, num_classes=3):\n",
                "        super().__init__()\n",
                "        layers = [nn.Linear(input_dim, units), nn.BatchNorm1d(units), nn.ReLU(), nn.Dropout(dropout)]\n",
                "        for _ in range(hidden_layers - 1):\n",
                "            layers += [nn.Linear(units, units), nn.BatchNorm1d(units), nn.ReLU(), nn.Dropout(dropout)]\n",
                "        layers.append(nn.Linear(units, num_classes))\n",
                "        self.network = nn.Sequential(*layers)\n",
                "    def forward(self, x): return self.network(x)\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self, input_dim, filters=32, kernel_size=2, dropout=0.3, num_classes=3):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv1d(1, filters, kernel_size)\n",
                "        self.bn1 = nn.BatchNorm1d(filters)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        conv_out = input_dim - kernel_size + 1\n",
                "        pool_out = conv_out // 2\n",
                "        self.pool = nn.MaxPool1d(2) if pool_out > 0 else nn.Identity()\n",
                "        if pool_out <= 0: pool_out = conv_out\n",
                "        self.flatten = nn.Flatten()\n",
                "        self.fc = nn.Linear(filters * pool_out, num_classes)\n",
                "    def forward(self, x):\n",
                "        x = self.dropout(self.pool(self.relu(self.bn1(self.conv1(x.unsqueeze(1))))))\n",
                "        return self.fc(self.flatten(x))\n",
                "\n",
                "INPUT_DIM = X_train.shape[1]\n",
                "NUM_CLASSES = len(le.classes_)\n",
                "print(f'Input dim: {INPUT_DIM}, Classes: {NUM_CLASSES}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. PyTorch Training with Checkpointing\n",
                "def train_pytorch_model(model, model_name, X_tr, y_tr, X_v, y_v, epochs=20, lr=0.001):\n",
                "    \"\"\"Train PyTorch model with AMP, return predictions on all sets.\"\"\"\n",
                "    model.to(DEVICE)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "    scaler = GradScaler(enabled=USE_AMP)\n",
                "\n",
                "    train_ds = SleepDataset(X_tr, y_tr)\n",
                "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
                "                          pin_memory=torch.cuda.is_available(), num_workers=2)\n",
                "\n",
                "    best_f1 = 0\n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        total_loss = 0\n",
                "        for Xb, yb in train_dl:\n",
                "            Xb, yb = Xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
                "            optimizer.zero_grad(set_to_none=True)\n",
                "            with autocast(enabled=USE_AMP):\n",
                "                loss = criterion(model(Xb), yb)\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "            total_loss += loss.item()\n",
                "\n",
                "        # Quick val check\n",
                "        val_dl = make_loaders((X_v, y_v))[0]\n",
                "        val_preds = predict_torch(model, val_dl)\n",
                "        val_f1 = f1_score(y_v, val_preds, average='weighted')\n",
                "\n",
                "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
                "            print(f'  Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_dl):.4f} | Val F1: {val_f1:.4f}')\n",
                "\n",
                "        if val_f1 > best_f1:\n",
                "            best_f1 = val_f1\n",
                "\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Train & Evaluate All Models\n",
                "Each model is trained, evaluated on Train/Val/Test, confusion matrices are plotted, and checkpoints are saved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8a. KNN\n",
                "print('='*50)\n",
                "print('Training KNN...')\n",
                "print('='*50)\n",
                "\n",
                "knn = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='euclidean', n_jobs=-1)\n",
                "knn.fit(X_train, y_train)\n",
                "\n",
                "knn_preds = {\n",
                "    'Train': knn.predict(X_train),\n",
                "    'Val':   knn.predict(X_val),\n",
                "    'Test':  knn.predict(X_test)\n",
                "}\n",
                "\n",
                "plot_confusion_matrices('KNN',\n",
                "    {'Train': y_train, 'Val': y_val, 'Test': y_test},\n",
                "    knn_preds, le.classes_, save_dir=CKPT_DIR)\n",
                "\n",
                "save_checkpoint(knn, 'KNN', CKPT_DIR, is_pytorch=False,\n",
                "    metrics={'test_acc': accuracy_score(y_test, knn_preds['Test']),\n",
                "             'test_f1': f1_score(y_test, knn_preds['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8b. SVM\n",
                "print('='*50)\n",
                "print('Training SVM...')\n",
                "print('='*50)\n",
                "\n",
                "# SVM on subsample for speed\n",
                "svm_sample = min(len(X_train), 20000)\n",
                "idx = np.random.choice(len(X_train), size=svm_sample, replace=False)\n",
                "svm = SVC(C=10.0, kernel='rbf', gamma='scale')\n",
                "svm.fit(X_train[idx], y_train[idx])\n",
                "\n",
                "svm_preds = {\n",
                "    'Train': svm.predict(X_train),\n",
                "    'Val':   svm.predict(X_val),\n",
                "    'Test':  svm.predict(X_test)\n",
                "}\n",
                "\n",
                "plot_confusion_matrices('SVM',\n",
                "    {'Train': y_train, 'Val': y_val, 'Test': y_test},\n",
                "    svm_preds, le.classes_, save_dir=CKPT_DIR)\n",
                "\n",
                "save_checkpoint(svm, 'SVM', CKPT_DIR, is_pytorch=False,\n",
                "    metrics={'test_acc': accuracy_score(y_test, svm_preds['Test']),\n",
                "             'test_f1': f1_score(y_test, svm_preds['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8c. Random Forest\n",
                "print('='*50)\n",
                "print('Training Random Forest...')\n",
                "print('='*50)\n",
                "\n",
                "rf = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42, n_jobs=-1)\n",
                "rf.fit(X_train, y_train)\n",
                "\n",
                "rf_preds = {\n",
                "    'Train': rf.predict(X_train),\n",
                "    'Val':   rf.predict(X_val),\n",
                "    'Test':  rf.predict(X_test)\n",
                "}\n",
                "\n",
                "plot_confusion_matrices('RF',\n",
                "    {'Train': y_train, 'Val': y_val, 'Test': y_test},\n",
                "    rf_preds, le.classes_, save_dir=CKPT_DIR)\n",
                "\n",
                "save_checkpoint(rf, 'RF', CKPT_DIR, is_pytorch=False,\n",
                "    metrics={'test_acc': accuracy_score(y_test, rf_preds['Test']),\n",
                "             'test_f1': f1_score(y_test, rf_preds['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8d. ANN\n",
                "print('='*50)\n",
                "print('Training ANN...')\n",
                "print('='*50)\n",
                "\n",
                "ann = ANN(INPUT_DIM, hidden_layers=2, units=128, dropout=0.3, num_classes=NUM_CLASSES)\n",
                "ann = train_pytorch_model(ann, 'ANN', X_train, y_train, X_val, y_val, epochs=20, lr=0.001)\n",
                "\n",
                "train_dl, val_dl, test_dl = make_loaders((X_train, y_train), (X_val, y_val), (X_test, y_test))\n",
                "ann_preds = {\n",
                "    'Train': predict_torch(ann, train_dl),\n",
                "    'Val':   predict_torch(ann, val_dl),\n",
                "    'Test':  predict_torch(ann, test_dl)\n",
                "}\n",
                "\n",
                "plot_confusion_matrices('ANN',\n",
                "    {'Train': y_train, 'Val': y_val, 'Test': y_test},\n",
                "    ann_preds, le.classes_, save_dir=CKPT_DIR)\n",
                "\n",
                "save_checkpoint(ann, 'ANN', CKPT_DIR, is_pytorch=True,\n",
                "    metrics={'test_acc': accuracy_score(y_test, ann_preds['Test']),\n",
                "             'test_f1': f1_score(y_test, ann_preds['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8e. CNN\n",
                "print('='*50)\n",
                "print('Training CNN...')\n",
                "print('='*50)\n",
                "\n",
                "cnn = CNN(INPUT_DIM, filters=32, kernel_size=2, dropout=0.3, num_classes=NUM_CLASSES)\n",
                "cnn = train_pytorch_model(cnn, 'CNN', X_train, y_train, X_val, y_val, epochs=20, lr=0.001)\n",
                "\n",
                "cnn_preds = {\n",
                "    'Train': predict_torch(cnn, train_dl),\n",
                "    'Val':   predict_torch(cnn, val_dl),\n",
                "    'Test':  predict_torch(cnn, test_dl)\n",
                "}\n",
                "\n",
                "plot_confusion_matrices('CNN',\n",
                "    {'Train': y_train, 'Val': y_val, 'Test': y_test},\n",
                "    cnn_preds, le.classes_, save_dir=CKPT_DIR)\n",
                "\n",
                "save_checkpoint(cnn, 'CNN', CKPT_DIR, is_pytorch=True,\n",
                "    metrics={'test_acc': accuracy_score(y_test, cnn_preds['Test']),\n",
                "             'test_f1': f1_score(y_test, cnn_preds['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Final Summary\n",
                "import pandas as pd\n",
                "\n",
                "summary = {}\n",
                "all_preds = {'KNN': knn_preds, 'SVM': svm_preds, 'RF': rf_preds, 'ANN': ann_preds, 'CNN': cnn_preds}\n",
                "all_y = {'Train': y_train, 'Val': y_val, 'Test': y_test}\n",
                "\n",
                "for model_name, preds in all_preds.items():\n",
                "    for split in ['Train', 'Val', 'Test']:\n",
                "        acc = accuracy_score(all_y[split], preds[split])\n",
                "        f1 = f1_score(all_y[split], preds[split], average='weighted')\n",
                "        summary[f'{model_name}_{split}'] = {'Accuracy': acc, 'F1': f1}\n",
                "\n",
                "df = pd.DataFrame(summary).T\n",
                "print('\\n' + '='*60)\n",
                "print('FULL RESULTS SUMMARY (all splits)')\n",
                "print('='*60)\n",
                "print(df.to_string(float_format='%.4f'))\n",
                "\n",
                "# Highlight overfitting detection\n",
                "print('\\n--- Overfitting Check (Train F1 - Test F1) ---')\n",
                "for m in ['KNN', 'SVM', 'RF', 'ANN', 'CNN']:\n",
                "    train_f1 = summary[f'{m}_Train']['F1']\n",
                "    test_f1 = summary[f'{m}_Test']['F1']\n",
                "    gap = train_f1 - test_f1\n",
                "    status = 'OK' if gap < 0.05 else 'OVERFIT' if gap > 0.1 else 'MODERATE'\n",
                "    print(f'  {m:4s}: Train={train_f1:.4f}  Test={test_f1:.4f}  Gap={gap:.4f}  [{status}]')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. List All Checkpoints\n",
                "print('\\nSaved checkpoints:')\n",
                "for f in sorted(os.listdir(CKPT_DIR)):\n",
                "    path = os.path.join(CKPT_DIR, f)\n",
                "    size = os.path.getsize(path) / (1024*1024)\n",
                "    print(f'  {f} ({size:.1f} MB)')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}