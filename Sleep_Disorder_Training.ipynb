{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sleep Disorder Classification - Advanced Architecture Training\n",
                "\n",
                "This notebook implements the training pipeline for 5 models: KNN, SVM, Random Forest, ANN, and CNN.\n",
                "It uses Optuna for hyperparameter optimization and saves the best models to Google Drive.\n",
                "\n",
                "## Quick Start\n",
                "By default, this notebook contains all the necessary source code embedded within it. \n",
                "However, if you have pushed the project code to GitHub, you can use the **'Clone from GitHub'** section below instead of writing the files manually."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup Environment\n",
                "!pip install optuna imbalanced-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Mount Google Drive\n",
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create specific folder for this project results\n",
                "project_path = '/content/drive/MyDrive/Sleep_Disorder_Project'\n",
                "os.makedirs(project_path, exist_ok=True)\n",
                "print(f\"Project contents will be saved to: {project_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Upload Dataset\n",
                "from google.colab import files\n",
                "import pandas as pd\n",
                "import io\n",
                "\n",
                "print(\"Please upload 'sleep_dataset.csv' from your local machine:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "for fn in uploaded.keys():\n",
                "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
                "      name=fn, length=len(uploaded[fn])))\n",
                "  # Ensure it is named correctly for the script\n",
                "  if fn != 'sleep_dataset.csv':\n",
                "      os.rename(fn, 'sleep_dataset.csv')\n",
                "      print(\"Renamed uploaded file to 'sleep_dataset.csv'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Option A: Clone from GitHub\n",
                "Use this if you have pushed the project code to a GitHub repository.\n",
                "Replace `YOUR_GITHUB_USERNAME/REPO_NAME` with your actual repository URL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment and run this cell to clone your repo\n",
                "!git clone https://github.com/Pradeep-1803/buhbuybnu.git\n",
                "%cd buhbuybnu"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Option B: Embedded Source Code\n",
                "Run these cells if you are NOT cloning from GitHub. This will create the necessary Python files directly in the Colab environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Create Source Directory\n",
                "os.makedirs('src', exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile src/data_loader.py\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from imblearn.over_sampling import SMOTE\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "def load_and_process_data(filepath):\n",
                "    print(f\"Loading data from {filepath}...\")\n",
                "    df = pd.read_csv(filepath)\n",
                "    \n",
                "    if 'Person ID' in df.columns:\n",
                "        df = df.drop(columns=['Person ID'])\n",
                "    \n",
                "    df['Sleep Disorder'] = df['Sleep Disorder'].fillna('None')\n",
                "    print(\"Class distribution before balancing:\")\n",
                "    print(df['Sleep Disorder'].value_counts())\n",
                "\n",
                "    if 'Blood Pressure' in df.columns:\n",
                "        df[['Systolic_BP', 'Diastolic_BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(float)\n",
                "        df = df.drop(columns=['Blood Pressure'])\n",
                "    \n",
                "    if 'BMI Category' in df.columns:\n",
                "        df['BMI Category'] = df['BMI Category'].replace({'Normal Weight': 'Normal'})\n",
                "\n",
                "    target_col = 'Sleep Disorder'\n",
                "    categorical_cols = ['Gender', 'Occupation', 'BMI Category']\n",
                "    numeric_cols = [col for col in df.columns if col not in categorical_cols + [target_col]]\n",
                "    \n",
                "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
                "    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
                "\n",
                "    preprocessor = ColumnTransformer(\n",
                "        transformers=[\n",
                "            ('num', numeric_transformer, numeric_cols),\n",
                "            ('cat', categorical_transformer, categorical_cols)\n",
                "        ]\n",
                "    )\n",
                "    \n",
                "    X = df.drop(columns=[target_col])\n",
                "    y = df[target_col]\n",
                "    \n",
                "    label_encoder = LabelEncoder()\n",
                "    y_encoded = label_encoder.fit_transform(y)\n",
                "    \n",
                "    print(\"Preprocessing features...\")\n",
                "    X_processed = preprocessor.fit_transform(X)\n",
                "    \n",
                "    X_train, X_test, y_train, y_test = train_test_split(\n",
                "        X_processed, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
                "    )\n",
                "    \n",
                "    print(\"Applying SMOTE to training data...\")\n",
                "    smote = SMOTE(random_state=42)\n",
                "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
                "    \n",
                "    return X_train_resampled, X_test, y_train_resampled, y_test, label_encoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile src/models.py\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from torch.utils.data import Dataset\n",
                "\n",
                "class SleepDataset(Dataset):\n",
                "    def __init__(self, X, y):\n",
                "        self.X = torch.tensor(X, dtype=torch.float32)\n",
                "        self.y = torch.tensor(y, dtype=torch.long)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.X)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.X[idx], self.y[idx]\n",
                "\n",
                "class ANN(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_layers=1, units_per_layer=24, dropout_rate=0.2, num_classes=3):\n",
                "        super(ANN, self).__init__()\n",
                "        layers = []\n",
                "        layers.append(nn.Linear(input_dim, units_per_layer))\n",
                "        layers.append(nn.BatchNorm1d(units_per_layer))\n",
                "        layers.append(nn.ReLU())\n",
                "        layers.append(nn.Dropout(dropout_rate))\n",
                "        \n",
                "        for _ in range(hidden_layers - 1):\n",
                "            layers.append(nn.Linear(units_per_layer, units_per_layer))\n",
                "            layers.append(nn.BatchNorm1d(units_per_layer))\n",
                "            layers.append(nn.ReLU())\n",
                "            layers.append(nn.Dropout(dropout_rate))\n",
                "            \n",
                "        layers.append(nn.Linear(units_per_layer, num_classes))\n",
                "        self.network = nn.Sequential(*layers)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.network(x)\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self, input_dim, filters=32, kernel_size=2, dropout_rate=0.3, num_classes=3):\n",
                "        super(CNN, self).__init__()\n",
                "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=filters, kernel_size=kernel_size)\n",
                "        self.bn1 = nn.BatchNorm1d(filters)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.dropout = nn.Dropout(dropout_rate)\n",
                "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
                "        \n",
                "        conv_out_size = input_dim - kernel_size + 1\n",
                "        pool_out_size = conv_out_size // 2\n",
                "        if pool_out_size <= 0:\n",
                "             self.pool = nn.Identity()\n",
                "             pool_out_size = conv_out_size\n",
                "\n",
                "        self.flatten = nn.Flatten()\n",
                "        self.fc = nn.Linear(filters * pool_out_size, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x.unsqueeze(1)\n",
                "        x = self.conv1(x)\n",
                "        x = self.bn1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.pool(x)\n",
                "        x = self.dropout(x)\n",
                "        x = self.flatten(x)\n",
                "        x = self.fc(x)\n",
                "        return x\n",
                "\n",
                "def get_sklearn_model(name, params):\n",
                "    if name == 'KNN':\n",
                "        return KNeighborsClassifier(**params, n_jobs=-1)\n",
                "    elif name == 'SVM':\n",
                "        return SVC(**params)\n",
                "    elif name == 'RF':\n",
                "        return RandomForestClassifier(**params, n_jobs=-1)\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown sklearn model: {name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Training and Evaluation Function\n",
                "import optuna\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from src.data_loader import load_and_process_data\n",
                "from src.models import SleepDataset, ANN, CNN, get_sklearn_model\n",
                "import joblib\n",
                "import os\n",
                "\n",
                "DATA_PATH = \"sleep_dataset.csv\"\n",
                "N_TRIALS = 20  # You can increase this for better optimization\n",
                "EPOCHS = 20\n",
                "BATCH_SIZE = 64\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "SAVE_DIR = '/content/drive/MyDrive/Sleep_Disorder_Project'\n",
                "\n",
                "def train_torch_model(model, train_loader, val_loader, epochs, lr):\n",
                "    model.to(DEVICE)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "    best_val_f1 = 0.0\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        for X_batch, y_batch in train_loader:\n",
                "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(X_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "        \n",
                "        model.eval()\n",
                "        all_preds, all_labels = [], []\n",
                "        with torch.no_grad():\n",
                "            for X_batch, y_batch in val_loader:\n",
                "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
                "                outputs = model(X_batch)\n",
                "                _, preds = torch.max(outputs, 1)\n",
                "                all_preds.extend(preds.cpu().numpy())\n",
                "                all_labels.extend(y_batch.cpu().numpy())\n",
                "        \n",
                "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
                "        if val_f1 > best_val_f1:\n",
                "            best_val_f1 = val_f1\n",
                "    return best_val_f1\n",
                "\n",
                "def objective(trial, model_name, X_train, X_val, y_train, y_val, input_dim):\n",
                "    if model_name == 'KNN':\n",
                "        params = {'n_neighbors': trial.suggest_int('n_neighbors', 3, 20), 'weights': trial.suggest_categorical('weights', ['uniform', 'distance'])}\n",
                "        model = get_sklearn_model('KNN', params)\n",
                "        model.fit(X_train, y_train)\n",
                "        return f1_score(y_val, model.predict(X_val), average='weighted')\n",
                "    elif model_name == 'SVM':\n",
                "        idx = np.random.choice(len(X_train), size=min(len(X_train), 5000), replace=False)\n",
                "        X_sub, y_sub = X_train[idx], y_train[idx]\n",
                "        params = {'C': trial.suggest_float('C', 0.1, 10.0, log=True), 'kernel': 'rbf'}\n",
                "        model = get_sklearn_model('SVM', params)\n",
                "        model.fit(X_sub, y_sub)\n",
                "        return f1_score(y_val, model.predict(X_val), average='weighted')\n",
                "    elif model_name == 'RF':\n",
                "        params = {'n_estimators': trial.suggest_int('n_estimators', 50, 150), 'max_depth': trial.suggest_int('max_depth', 5, 20)}\n",
                "        model = get_sklearn_model('RF', params)\n",
                "        model.fit(X_train, y_train)\n",
                "        return f1_score(y_val, model.predict(X_val), average='weighted')\n",
                "    elif model_name == 'ANN' or model_name == 'CNN':\n",
                "        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
                "        dropout = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
                "        train_ds, val_ds = SleepDataset(X_train, y_train), SleepDataset(X_val, y_val)\n",
                "        train_dl, val_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True), DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
                "        if model_name == 'ANN':\n",
                "            model = ANN(input_dim, trial.suggest_int('hidden_layers', 1, 3), trial.suggest_int('units_per_layer', 16, 128), dropout)\n",
                "        else:\n",
                "            model = CNN(input_dim, trial.suggest_int('filters', 16, 64), trial.suggest_int('kernel_size', 2, 3), dropout)\n",
                "        return train_torch_model(model, train_dl, val_dl, 10, lr)\n",
                "\n",
                "def run_all():\n",
                "    X_full, X_test, y_full, y_test, le = load_and_process_data(DATA_PATH)\n",
                "    input_dim = X_full.shape[1]\n",
                "    \n",
                "    # Validation split for Optuna\n",
                "    test_split_idx = int(0.8 * len(X_full))\n",
                "    X_train_opt, X_val_opt = X_full[:test_split_idx], X_full[test_split_idx:]\n",
                "    y_train_opt, y_val_opt = y_full[:test_split_idx], y_full[test_split_idx:]\n",
                "    \n",
                "    models = ['KNN', 'SVM', 'RF', 'ANN', 'CNN']\n",
                "    results = {}\n",
                "\n",
                "    for m in models:\n",
                "        print(f\"\\nOptimizing {m}...\")\n",
                "        study = optuna.create_study(direction='maximize')\n",
                "        study.optimize(lambda t: objective(t, m, X_train_opt, X_val_opt, y_train_opt, y_val_opt, input_dim), n_trials=N_TRIALS)\n",
                "        print(f\"Best Params: {study.best_params}\")\n",
                "        \n",
                "        # Retrain Best Model\n",
                "        print(f\"Training final {m}...\")\n",
                "        if m in ['KNN', 'SVM', 'RF']:\n",
                "            model = get_sklearn_model(m, study.best_params)\n",
                "            model.fit(X_full, y_full)\n",
                "            preds = model.predict(X_test)\n",
                "            # Save Model\n",
                "            model_path = os.path.join(SAVE_DIR, f'{m}_best_model.joblib')\n",
                "            joblib.dump(model, model_path)\n",
                "            print(f\"Saved {m} to {model_path}\")\n",
                "        else:\n",
                "            p = study.best_params\n",
                "            if m == 'ANN':\n",
                "                model = ANN(input_dim, p['hidden_layers'], p['units_per_layer'], p['dropout_rate'])\n",
                "            else:\n",
                "                model = CNN(input_dim, p['filters'], p['kernel_size'], p['dropout_rate'])\n",
                "            \n",
                "            ds_train, ds_test = SleepDataset(X_full, y_full), SleepDataset(X_test, y_test)\n",
                "            dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
                "            dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE)\n",
                "            \n",
                "            train_torch_model(model, dl_train, dl_test, epochs=EPOCHS, lr=p['lr'])\n",
                "            \n",
                "            # Save PyTorch Model\n",
                "            model_path = os.path.join(SAVE_DIR, f'{m}_best_model.pth')\n",
                "            torch.save(model.state_dict(), model_path)\n",
                "            print(f\"Saved {m} to {model_path}\")\n",
                "            \n",
                "            model.eval()\n",
                "            preds = []\n",
                "            with torch.no_grad():\n",
                "                for Xb, _ in dl_test:\n",
                "                    out = model(Xb.to(DEVICE))\n",
                "                    preds.extend(torch.max(out, 1)[1].cpu().numpy())\n",
                "        \n",
                "        acc = accuracy_score(y_test, preds)\n",
                "        f1 = f1_score(y_test, preds, average='weighted')\n",
                "        results[m] = {'Accuracy': acc, 'F1': f1}\n",
                "        print(f\"{m} Result -> Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
                "        \n",
                "    print(\"\\n--- Final Results ---\")\n",
                "    print(pd.DataFrame(results).T)\n",
                "\n",
                "run_all()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}