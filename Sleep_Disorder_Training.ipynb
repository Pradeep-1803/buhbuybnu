{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sleep Disorder Classification - GPU-Accelerated Training\n",
                "\n",
                "Trains 5 models (KNN, SVM, RF, ANN, CNN) with Optuna optimization.\n",
                "\n",
                "**GPU Features**: Mixed precision (AMP), batch size 512, pin_memory, async CUDA transfers.\n",
                "\n",
                "> **Important**: Go to **Runtime → Change runtime type → T4 GPU** before running!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Verify GPU\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n",
                "    print(f'CUDA version: {torch.version.cuda}')\n",
                "else:\n",
                "    print('WARNING: No GPU detected!')\n",
                "    print('Go to Runtime -> Change runtime type -> T4 GPU')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install dependencies\n",
                "!pip install -q optuna imbalanced-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Mount Google Drive\n",
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "project_path = '/content/drive/MyDrive/Sleep_Disorder_Project'\n",
                "os.makedirs(project_path, exist_ok=True)\n",
                "print(f\"Models will be saved to: {project_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Upload Dataset\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Upload 'sleep_dataset.csv':\")\n",
                "uploaded = files.upload()\n",
                "for fn in uploaded.keys():\n",
                "    print(f'Uploaded \"{fn}\" ({len(uploaded[fn])} bytes)')\n",
                "    if fn != 'sleep_dataset.csv':\n",
                "        os.rename(fn, 'sleep_dataset.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Source Code Setup\n",
                "**Option A**: Clone from GitHub (recommended)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to clone from GitHub\n",
                "# !git clone https://github.com/Pradeep-1803/buhbuybnu.git\n",
                "# %cd buhbuybnu"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Option B**: Embedded code (run cells below if NOT cloning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs('src', exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile src/data_loader.py\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from imblearn.over_sampling import SMOTE\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "\n",
                "def load_and_process_data(filepath):\n",
                "    \"\"\"Loads and preprocesses the sleep dataset. Returns data WITHOUT SMOTE.\"\"\"\n",
                "    print(f\"Loading data from {filepath}...\")\n",
                "    df = pd.read_csv(filepath)\n",
                "    \n",
                "    if 'Person ID' in df.columns:\n",
                "        df = df.drop(columns=['Person ID'])\n",
                "    \n",
                "    df['Sleep Disorder'] = df['Sleep Disorder'].fillna('None')\n",
                "    print(\"Class distribution:\")\n",
                "    print(df['Sleep Disorder'].value_counts())\n",
                "\n",
                "    if 'Blood Pressure' in df.columns:\n",
                "        df[['Systolic_BP', 'Diastolic_BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(float)\n",
                "        df = df.drop(columns=['Blood Pressure'])\n",
                "    \n",
                "    if 'BMI Category' in df.columns:\n",
                "        df['BMI Category'] = df['BMI Category'].replace({'Normal Weight': 'Normal'})\n",
                "\n",
                "    target_col = 'Sleep Disorder'\n",
                "    categorical_cols = ['Gender', 'Occupation', 'BMI Category']\n",
                "    numeric_cols = [col for col in df.columns if col not in categorical_cols + [target_col]]\n",
                "    \n",
                "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
                "    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
                "\n",
                "    preprocessor = ColumnTransformer(\n",
                "        transformers=[\n",
                "            ('num', numeric_transformer, numeric_cols),\n",
                "            ('cat', categorical_transformer, categorical_cols)\n",
                "        ]\n",
                "    )\n",
                "    \n",
                "    X = df.drop(columns=[target_col])\n",
                "    y = df[target_col]\n",
                "    \n",
                "    label_encoder = LabelEncoder()\n",
                "    y_encoded = label_encoder.fit_transform(y)\n",
                "    \n",
                "    print(\"Preprocessing features...\")\n",
                "    X_processed = preprocessor.fit_transform(X)\n",
                "    \n",
                "    X_train, X_test, y_train, y_test = train_test_split(\n",
                "        X_processed, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
                "    )\n",
                "    \n",
                "    print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
                "    return X_train, X_test, y_train, y_test, label_encoder\n",
                "\n",
                "\n",
                "def apply_smote(X, y):\n",
                "    \"\"\"Apply SMOTE to balance classes. Use only on training data.\"\"\"\n",
                "    smote = SMOTE(random_state=42)\n",
                "    X_res, y_res = smote.fit_resample(X, y)\n",
                "    return X_res, y_res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile src/models.py\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from torch.utils.data import Dataset\n",
                "\n",
                "class SleepDataset(Dataset):\n",
                "    def __init__(self, X, y):\n",
                "        self.X = torch.tensor(X, dtype=torch.float32)\n",
                "        self.y = torch.tensor(y, dtype=torch.long)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.X)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.X[idx], self.y[idx]\n",
                "\n",
                "class ANN(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_layers=1, units_per_layer=24, dropout_rate=0.2, num_classes=3):\n",
                "        super(ANN, self).__init__()\n",
                "        layers = []\n",
                "        layers.append(nn.Linear(input_dim, units_per_layer))\n",
                "        layers.append(nn.BatchNorm1d(units_per_layer))\n",
                "        layers.append(nn.ReLU())\n",
                "        layers.append(nn.Dropout(dropout_rate))\n",
                "        for _ in range(hidden_layers - 1):\n",
                "            layers.append(nn.Linear(units_per_layer, units_per_layer))\n",
                "            layers.append(nn.BatchNorm1d(units_per_layer))\n",
                "            layers.append(nn.ReLU())\n",
                "            layers.append(nn.Dropout(dropout_rate))\n",
                "        layers.append(nn.Linear(units_per_layer, num_classes))\n",
                "        self.network = nn.Sequential(*layers)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.network(x)\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self, input_dim, filters=32, kernel_size=2, dropout_rate=0.3, num_classes=3):\n",
                "        super(CNN, self).__init__()\n",
                "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=filters, kernel_size=kernel_size)\n",
                "        self.bn1 = nn.BatchNorm1d(filters)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.dropout = nn.Dropout(dropout_rate)\n",
                "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
                "        conv_out_size = input_dim - kernel_size + 1\n",
                "        pool_out_size = conv_out_size // 2\n",
                "        if pool_out_size <= 0:\n",
                "            self.pool = nn.Identity()\n",
                "            pool_out_size = conv_out_size\n",
                "        self.flatten = nn.Flatten()\n",
                "        self.fc = nn.Linear(filters * pool_out_size, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x.unsqueeze(1)\n",
                "        x = self.conv1(x)\n",
                "        x = self.bn1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.pool(x)\n",
                "        x = self.dropout(x)\n",
                "        x = self.flatten(x)\n",
                "        x = self.fc(x)\n",
                "        return x\n",
                "\n",
                "def get_sklearn_model(name, params):\n",
                "    if name == 'KNN':\n",
                "        return KNeighborsClassifier(**params, n_jobs=-1)\n",
                "    elif name == 'SVM':\n",
                "        return SVC(**params)\n",
                "    elif name == 'RF':\n",
                "        return RandomForestClassifier(**params, n_jobs=-1)\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown sklearn model: {name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. GPU-Accelerated Training Pipeline\n",
                "import optuna\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
                "from sklearn.model_selection import train_test_split\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from src.data_loader import load_and_process_data, apply_smote\n",
                "from src.models import SleepDataset, ANN, CNN, get_sklearn_model\n",
                "import joblib\n",
                "import os\n",
                "\n",
                "DATA_PATH = \"sleep_dataset.csv\"\n",
                "N_TRIALS = 20\n",
                "EPOCHS = 20\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "USE_AMP = torch.cuda.is_available()\n",
                "BATCH_SIZE = 512 if torch.cuda.is_available() else 64\n",
                "SAVE_DIR = '/content/drive/MyDrive/Sleep_Disorder_Project'\n",
                "\n",
                "\n",
                "def train_torch_model(model, train_loader, val_loader, epochs, lr):\n",
                "    model.to(DEVICE)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "    scaler = GradScaler(enabled=USE_AMP)\n",
                "    best_val_f1 = 0.0\n",
                "\n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        for X_batch, y_batch in train_loader:\n",
                "            X_batch = X_batch.to(DEVICE, non_blocking=True)\n",
                "            y_batch = y_batch.to(DEVICE, non_blocking=True)\n",
                "            optimizer.zero_grad(set_to_none=True)\n",
                "            with autocast(enabled=USE_AMP):\n",
                "                outputs = model(X_batch)\n",
                "                loss = criterion(outputs, y_batch)\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "\n",
                "        model.eval()\n",
                "        all_preds, all_labels = [], []\n",
                "        with torch.no_grad(), autocast(enabled=USE_AMP):\n",
                "            for X_batch, y_batch in val_loader:\n",
                "                X_batch = X_batch.to(DEVICE, non_blocking=True)\n",
                "                outputs = model(X_batch)\n",
                "                _, preds = torch.max(outputs, 1)\n",
                "                all_preds.extend(preds.cpu().numpy())\n",
                "                all_labels.extend(y_batch.numpy())\n",
                "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
                "        if val_f1 > best_val_f1:\n",
                "            best_val_f1 = val_f1\n",
                "    return best_val_f1\n",
                "\n",
                "\n",
                "def make_loaders(X_train, y_train, X_val, y_val):\n",
                "    pin = torch.cuda.is_available()\n",
                "    train_ds = SleepDataset(X_train, y_train)\n",
                "    val_ds = SleepDataset(X_val, y_val)\n",
                "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=pin, num_workers=2)\n",
                "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, pin_memory=pin, num_workers=2)\n",
                "    return train_dl, val_dl\n",
                "\n",
                "\n",
                "def objective(trial, model_name, X_train_raw, X_val, y_train_raw, y_val, input_dim):\n",
                "    X_train, y_train = apply_smote(X_train_raw, y_train_raw)\n",
                "\n",
                "    if model_name == 'KNN':\n",
                "        params = {\n",
                "            'n_neighbors': trial.suggest_int('n_neighbors', 3, 15),\n",
                "            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
                "            'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan'])\n",
                "        }\n",
                "        model = get_sklearn_model('KNN', params)\n",
                "        model.fit(X_train, y_train)\n",
                "        return f1_score(y_val, model.predict(X_val), average='weighted')\n",
                "\n",
                "    elif model_name == 'SVM':\n",
                "        subsample_size = min(len(X_train), 20000)\n",
                "        idx = np.random.choice(len(X_train), size=subsample_size, replace=False)\n",
                "        X_sub, y_sub = X_train[idx], y_train[idx]\n",
                "        params = {\n",
                "            'C': trial.suggest_float('C', 0.1, 100.0, log=True),\n",
                "            'kernel': trial.suggest_categorical('kernel', ['linear', 'rbf']),\n",
                "            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
                "        }\n",
                "        model = get_sklearn_model('SVM', params)\n",
                "        model.fit(X_sub, y_sub)\n",
                "        return f1_score(y_val, model.predict(X_val), average='weighted')\n",
                "\n",
                "    elif model_name == 'RF':\n",
                "        params = {\n",
                "            'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
                "            'max_depth': trial.suggest_int('max_depth', 10, 50),\n",
                "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
                "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4)\n",
                "        }\n",
                "        model = get_sklearn_model('RF', params)\n",
                "        model.fit(X_train, y_train)\n",
                "        return f1_score(y_val, model.predict(X_val), average='weighted')\n",
                "\n",
                "    elif model_name in ['ANN', 'CNN']:\n",
                "        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
                "        dropout = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
                "        train_dl, val_dl = make_loaders(X_train, y_train, X_val, y_val)\n",
                "        if model_name == 'ANN':\n",
                "            model = ANN(input_dim,\n",
                "                        trial.suggest_int('hidden_layers', 1, 3),\n",
                "                        trial.suggest_int('units_per_layer', 32, 256),\n",
                "                        dropout)\n",
                "        else:\n",
                "            model = CNN(input_dim,\n",
                "                        trial.suggest_int('filters', 16, 64),\n",
                "                        trial.suggest_int('kernel_size', 2, 3),\n",
                "                        dropout)\n",
                "        return train_torch_model(model, train_dl, val_dl, 10, lr)\n",
                "\n",
                "\n",
                "def run_all():\n",
                "    print(f\"Device: {DEVICE}\")\n",
                "    if torch.cuda.is_available():\n",
                "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "        print(f\"Mixed Precision: Enabled | Batch Size: {BATCH_SIZE}\")\n",
                "    else:\n",
                "        print(\"WARNING: No GPU. Training will be slower.\")\n",
                "\n",
                "    X_train_raw, X_test, y_train_raw, y_test, le = load_and_process_data(DATA_PATH)\n",
                "    input_dim = X_train_raw.shape[1]\n",
                "    print(f\"Input dim: {input_dim}\")\n",
                "\n",
                "    X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
                "        X_train_raw, y_train_raw, test_size=0.2, random_state=42, stratify=y_train_raw\n",
                "    )\n",
                "    print(f\"Opt split: train={X_opt_train.shape[0]}, val={X_opt_val.shape[0]}\")\n",
                "\n",
                "    models = ['KNN', 'SVM', 'RF', 'ANN', 'CNN']\n",
                "    results = {}\n",
                "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
                "\n",
                "    for m in models:\n",
                "        print(f\"\\n{'='*50}\")\n",
                "        print(f\"Optimizing {m}...\")\n",
                "        print(f\"{'='*50}\")\n",
                "\n",
                "        study = optuna.create_study(direction='maximize')\n",
                "        study.optimize(\n",
                "            lambda t: objective(t, m, X_opt_train, X_opt_val, y_opt_train, y_opt_val, input_dim),\n",
                "            n_trials=N_TRIALS\n",
                "        )\n",
                "        print(f\"Best Params: {study.best_params}\")\n",
                "        print(f\"Best Opt F1: {study.best_value:.4f}\")\n",
                "\n",
                "        print(f\"Retraining final {m}...\")\n",
                "        X_train_full, y_train_full = apply_smote(X_train_raw, y_train_raw)\n",
                "\n",
                "        if m in ['KNN', 'SVM', 'RF']:\n",
                "            best_model = get_sklearn_model(m, study.best_params)\n",
                "            best_model.fit(X_train_full, y_train_full)\n",
                "            preds = best_model.predict(X_test)\n",
                "            model_path = os.path.join(SAVE_DIR, f'{m}_best_model.joblib')\n",
                "            joblib.dump(best_model, model_path)\n",
                "            print(f\"Saved {m} to {model_path}\")\n",
                "        else:\n",
                "            p = study.best_params\n",
                "            if m == 'ANN':\n",
                "                best_model = ANN(input_dim, p['hidden_layers'], p['units_per_layer'], p['dropout_rate'])\n",
                "            else:\n",
                "                best_model = CNN(input_dim, p['filters'], p['kernel_size'], p['dropout_rate'])\n",
                "            dl_train, dl_test = make_loaders(X_train_full, y_train_full, X_test, y_test)\n",
                "            train_torch_model(best_model, dl_train, dl_test, epochs=EPOCHS, lr=p['lr'])\n",
                "            model_path = os.path.join(SAVE_DIR, f'{m}_best_model.pth')\n",
                "            torch.save(best_model.state_dict(), model_path)\n",
                "            print(f\"Saved {m} to {model_path}\")\n",
                "            best_model.eval()\n",
                "            preds = []\n",
                "            with torch.no_grad(), autocast(enabled=USE_AMP):\n",
                "                for Xb, _ in dl_test:\n",
                "                    out = best_model(Xb.to(DEVICE, non_blocking=True))\n",
                "                    preds.extend(torch.max(out, 1)[1].cpu().numpy())\n",
                "\n",
                "        acc = accuracy_score(y_test, preds)\n",
                "        prec = precision_score(y_test, preds, average='weighted')\n",
                "        rec = recall_score(y_test, preds, average='weighted')\n",
                "        f1 = f1_score(y_test, preds, average='weighted')\n",
                "        results[m] = {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1}\n",
                "        print(f\"\\n{m} Test -> Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
                "        print(classification_report(y_test, preds, target_names=le.classes_))\n",
                "\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"FINAL RESULTS SUMMARY\")\n",
                "    print(\"=\"*60)\n",
                "    print(pd.DataFrame(results).T.to_string())\n",
                "\n",
                "\n",
                "run_all()"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
