{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sleep Disorder Classification — Training with Checkpoints & Confusion Matrices\n",
                "\n",
                "**Loads preprocessed `.npy` files from Google Drive** (creates them from CSV if they don't exist yet).\n",
                "\n",
                "- Separate confusion matrices for **Train / Validation / Test** per model\n",
                "- Versioned **checkpoints** saved to Drive for iterative improvement\n",
                "\n",
                "> Set runtime to **T4 GPU**: Runtime → Change runtime type"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup\n",
                "!pip install -q optuna imbalanced-learn seaborn\n",
                "\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    print(f'GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB)')\n",
                "else:\n",
                "    print('WARNING: No GPU! Runtime -> Change runtime type -> T4 GPU')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Mount Drive\n",
                "from google.colab import drive\n",
                "import numpy as np\n",
                "import joblib\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "DATA_DIR = '/content/drive/MyDrive/Sleep_Disorder_Project'\n",
                "CKPT_DIR = os.path.join(DATA_DIR, 'checkpoints')\n",
                "os.makedirs(CKPT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load or Create Preprocessed Data\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from imblearn.over_sampling import SMOTE\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "npy_check = os.path.join(DATA_DIR, 'X_train_full.npy')\n",
                "\n",
                "if os.path.exists(npy_check):\n",
                "    print('Found existing .npy files. Loading...')\n",
                "    X_train_full = np.load(os.path.join(DATA_DIR, 'X_train_full.npy'))\n",
                "    y_train_full = np.load(os.path.join(DATA_DIR, 'y_train_full.npy'))\n",
                "    X_test = np.load(os.path.join(DATA_DIR, 'X_test.npy'))\n",
                "    y_test = np.load(os.path.join(DATA_DIR, 'y_test.npy'))\n",
                "    le = joblib.load(os.path.join(DATA_DIR, 'label_encoder.joblib'))\n",
                "else:\n",
                "    print('.npy files not found. Preprocessing from CSV...')\n",
                "    # Upload CSV if not present\n",
                "    csv_path = 'sleep_dataset.csv'\n",
                "    if not os.path.exists(csv_path):\n",
                "        from google.colab import files\n",
                "        print('Upload sleep_dataset.csv:')\n",
                "        uploaded = files.upload()\n",
                "        for fn in uploaded.keys():\n",
                "            if fn != csv_path:\n",
                "                os.rename(fn, csv_path)\n",
                "\n",
                "    df = pd.read_csv(csv_path)\n",
                "    if 'Person ID' in df.columns:\n",
                "        df = df.drop(columns=['Person ID'])\n",
                "    df['Sleep Disorder'] = df['Sleep Disorder'].fillna('None')\n",
                "    print('Class distribution:')\n",
                "    print(df['Sleep Disorder'].value_counts())\n",
                "\n",
                "    if 'Blood Pressure' in df.columns:\n",
                "        df[['Systolic_BP', 'Diastolic_BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(float)\n",
                "        df = df.drop(columns=['Blood Pressure'])\n",
                "    if 'BMI Category' in df.columns:\n",
                "        df['BMI Category'] = df['BMI Category'].replace({'Normal Weight': 'Normal'})\n",
                "\n",
                "    target_col = 'Sleep Disorder'\n",
                "    cat_cols = ['Gender', 'Occupation', 'BMI Category']\n",
                "    num_cols = [c for c in df.columns if c not in cat_cols + [target_col]]\n",
                "\n",
                "    preprocessor = ColumnTransformer([\n",
                "        ('num', Pipeline([('scaler', StandardScaler())]), num_cols),\n",
                "        ('cat', Pipeline([('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), cat_cols)\n",
                "    ])\n",
                "\n",
                "    X = df.drop(columns=[target_col])\n",
                "    le = LabelEncoder()\n",
                "    y = le.fit_transform(df[target_col])\n",
                "    X_processed = preprocessor.fit_transform(X)\n",
                "\n",
                "    X_train_raw, X_test, y_train_raw, y_test = train_test_split(\n",
                "        X_processed, y, test_size=0.3, random_state=42, stratify=y)\n",
                "\n",
                "    print('Applying SMOTE...')\n",
                "    smote = SMOTE(random_state=42)\n",
                "    X_train_full, y_train_full = smote.fit_resample(X_train_raw, y_train_raw)\n",
                "\n",
                "    # Save to Drive\n",
                "    np.save(os.path.join(DATA_DIR, 'X_train_full.npy'), X_train_full)\n",
                "    np.save(os.path.join(DATA_DIR, 'y_train_full.npy'), y_train_full)\n",
                "    np.save(os.path.join(DATA_DIR, 'X_test.npy'), X_test)\n",
                "    np.save(os.path.join(DATA_DIR, 'y_test.npy'), y_test)\n",
                "    joblib.dump(le, os.path.join(DATA_DIR, 'label_encoder.joblib'))\n",
                "    print(f'Saved .npy files to {DATA_DIR}')\n",
                "\n",
                "print(f'Train: {X_train_full.shape}, Test: {X_test.shape}')\n",
                "print(f'Classes: {list(le.classes_)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Train/Val Split\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full)\n",
                "print(f'Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Confusion Matrix Plotting\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
                "\n",
                "def plot_confusion_matrices(model_name, y_sets, pred_sets, class_names, save_dir=None):\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
                "    fig.suptitle(f'{model_name} — Confusion Matrices', fontsize=16, fontweight='bold')\n",
                "    for ax, split in zip(axes, ['Train', 'Val', 'Test']):\n",
                "        cm = confusion_matrix(y_sets[split], pred_sets[split])\n",
                "        acc = accuracy_score(y_sets[split], pred_sets[split])\n",
                "        f1 = f1_score(y_sets[split], pred_sets[split], average='weighted')\n",
                "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "                    xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
                "        ax.set_title(f'{split}\\nAcc: {acc:.4f} | F1: {f1:.4f}')\n",
                "        ax.set_ylabel('True'); ax.set_xlabel('Predicted')\n",
                "    plt.tight_layout()\n",
                "    if save_dir:\n",
                "        path = os.path.join(save_dir, f'{model_name}_confusion.png')\n",
                "        plt.savefig(path, dpi=150, bbox_inches='tight')\n",
                "        print(f'Saved: {path}')\n",
                "    plt.show()\n",
                "    for split in ['Train', 'Val', 'Test']:\n",
                "        print(f'\\n--- {model_name} {split} ---')\n",
                "        print(classification_report(y_sets[split], pred_sets[split], target_names=class_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Checkpoint & PyTorch Utilities\n",
                "import torch, torch.nn as nn, torch.optim as optim\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "import glob\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "USE_AMP = torch.cuda.is_available()\n",
                "BATCH_SIZE = 512 if torch.cuda.is_available() else 64\n",
                "\n",
                "class SleepDataset(Dataset):\n",
                "    def __init__(self, X, y):\n",
                "        self.X = torch.tensor(X, dtype=torch.float32)\n",
                "        self.y = torch.tensor(y, dtype=torch.long)\n",
                "    def __len__(self): return len(self.X)\n",
                "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
                "\n",
                "def get_ckpt_ver(name, d):\n",
                "    files = glob.glob(os.path.join(d, f'{name}_v*.*'))\n",
                "    if not files: return 1\n",
                "    nums = []\n",
                "    for f in files:\n",
                "        try: nums.append(int(os.path.basename(f).split('_v')[1].split('.')[0]))\n",
                "        except: pass\n",
                "    return max(nums)+1 if nums else 1\n",
                "\n",
                "def save_ckpt(model, name, ckpt_dir, pytorch=False, metrics=None):\n",
                "    ver = get_ckpt_ver(name, ckpt_dir)\n",
                "    if pytorch:\n",
                "        path = os.path.join(ckpt_dir, f'{name}_v{ver}.pth')\n",
                "        torch.save({'state_dict': model.state_dict(), 'v': ver, 'metrics': metrics or {}}, path)\n",
                "    else:\n",
                "        path = os.path.join(ckpt_dir, f'{name}_v{ver}.joblib')\n",
                "        joblib.dump({'model': model, 'v': ver, 'metrics': metrics or {}}, path)\n",
                "    print(f'Checkpoint: {path} (v{ver})')\n",
                "\n",
                "def make_loader(X, y):\n",
                "    return DataLoader(SleepDataset(X, y), batch_size=BATCH_SIZE,\n",
                "                      pin_memory=torch.cuda.is_available(), num_workers=2)\n",
                "\n",
                "def predict_torch(model, dl):\n",
                "    model.eval()\n",
                "    preds = []\n",
                "    with torch.no_grad(), autocast(enabled=USE_AMP):\n",
                "        for Xb, _ in dl:\n",
                "            preds.extend(torch.max(model(Xb.to(DEVICE, non_blocking=True)), 1)[1].cpu().numpy())\n",
                "    return np.array(preds)\n",
                "\n",
                "print(f'Device: {DEVICE} | Batch: {BATCH_SIZE}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Model Definitions\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "class ANN(nn.Module):\n",
                "    def __init__(self, dim, layers=2, units=128, drop=0.3, nc=3):\n",
                "        super().__init__()\n",
                "        l = [nn.Linear(dim, units), nn.BatchNorm1d(units), nn.ReLU(), nn.Dropout(drop)]\n",
                "        for _ in range(layers-1):\n",
                "            l += [nn.Linear(units, units), nn.BatchNorm1d(units), nn.ReLU(), nn.Dropout(drop)]\n",
                "        l.append(nn.Linear(units, nc))\n",
                "        self.net = nn.Sequential(*l)\n",
                "    def forward(self, x): return self.net(x)\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self, dim, filt=32, ks=2, drop=0.3, nc=3):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Conv1d(1, filt, ks)\n",
                "        self.bn = nn.BatchNorm1d(filt)\n",
                "        co = dim - ks + 1; po = co // 2\n",
                "        self.pool = nn.MaxPool1d(2) if po > 0 else nn.Identity()\n",
                "        if po <= 0: po = co\n",
                "        self.drop = nn.Dropout(drop)\n",
                "        self.fc = nn.Linear(filt * po, nc)\n",
                "    def forward(self, x):\n",
                "        x = self.drop(self.pool(torch.relu(self.bn(self.conv(x.unsqueeze(1))))))\n",
                "        return self.fc(x.flatten(1))\n",
                "\n",
                "INPUT_DIM = X_train.shape[1]\n",
                "NC = len(le.classes_)\n",
                "print(f'Input: {INPUT_DIM}, Classes: {NC}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. PyTorch Training Function\n",
                "def train_model(model, name, X_tr, y_tr, X_v, y_v, epochs=20, lr=0.001):\n",
                "    model.to(DEVICE)\n",
                "    crit = nn.CrossEntropyLoss()\n",
                "    opt = optim.Adam(model.parameters(), lr=lr)\n",
                "    scaler = GradScaler(enabled=USE_AMP)\n",
                "    dl = DataLoader(SleepDataset(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True,\n",
                "                    pin_memory=torch.cuda.is_available(), num_workers=2)\n",
                "    for ep in range(epochs):\n",
                "        model.train(); loss_sum = 0\n",
                "        for Xb, yb in dl:\n",
                "            Xb, yb = Xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
                "            opt.zero_grad(set_to_none=True)\n",
                "            with autocast(enabled=USE_AMP): loss = crit(model(Xb), yb)\n",
                "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
                "            loss_sum += loss.item()\n",
                "        vp = predict_torch(model, make_loader(X_v, y_v))\n",
                "        vf1 = f1_score(y_v, vp, average='weighted')\n",
                "        if (ep+1) % 5 == 0 or ep == 0:\n",
                "            print(f'  Ep {ep+1}/{epochs} | Loss: {loss_sum/len(dl):.4f} | Val F1: {vf1:.4f}')\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Train & Evaluate All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9a. KNN\n",
                "print('='*50, '\\nKNN\\n', '='*50)\n",
                "knn = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='euclidean', n_jobs=-1)\n",
                "knn.fit(X_train, y_train)\n",
                "kp = {'Train': knn.predict(X_train), 'Val': knn.predict(X_val), 'Test': knn.predict(X_test)}\n",
                "ys = {'Train': y_train, 'Val': y_val, 'Test': y_test}\n",
                "plot_confusion_matrices('KNN', ys, kp, le.classes_, CKPT_DIR)\n",
                "save_ckpt(knn, 'KNN', CKPT_DIR, metrics={'test_f1': f1_score(y_test, kp['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9b. SVM\n",
                "print('='*50, '\\nSVM\\n', '='*50)\n",
                "n = min(len(X_train), 20000)\n",
                "idx = np.random.choice(len(X_train), size=n, replace=False)\n",
                "svm = SVC(C=10.0, kernel='rbf', gamma='scale')\n",
                "svm.fit(X_train[idx], y_train[idx])\n",
                "sp = {'Train': svm.predict(X_train), 'Val': svm.predict(X_val), 'Test': svm.predict(X_test)}\n",
                "plot_confusion_matrices('SVM', ys, sp, le.classes_, CKPT_DIR)\n",
                "save_ckpt(svm, 'SVM', CKPT_DIR, metrics={'test_f1': f1_score(y_test, sp['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9c. Random Forest\n",
                "print('='*50, '\\nRandom Forest\\n', '='*50)\n",
                "rf = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42, n_jobs=-1)\n",
                "rf.fit(X_train, y_train)\n",
                "rp = {'Train': rf.predict(X_train), 'Val': rf.predict(X_val), 'Test': rf.predict(X_test)}\n",
                "plot_confusion_matrices('RF', ys, rp, le.classes_, CKPT_DIR)\n",
                "save_ckpt(rf, 'RF', CKPT_DIR, metrics={'test_f1': f1_score(y_test, rp['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9d. ANN\n",
                "print('='*50, '\\nANN\\n', '='*50)\n",
                "ann = ANN(INPUT_DIM, layers=2, units=128, drop=0.3, nc=NC)\n",
                "ann = train_model(ann, 'ANN', X_train, y_train, X_val, y_val, epochs=20, lr=0.001)\n",
                "tdl, vdl, sdl = make_loader(X_train, y_train), make_loader(X_val, y_val), make_loader(X_test, y_test)\n",
                "ap = {'Train': predict_torch(ann, tdl), 'Val': predict_torch(ann, vdl), 'Test': predict_torch(ann, sdl)}\n",
                "plot_confusion_matrices('ANN', ys, ap, le.classes_, CKPT_DIR)\n",
                "save_ckpt(ann, 'ANN', CKPT_DIR, pytorch=True, metrics={'test_f1': f1_score(y_test, ap['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9e. CNN\n",
                "print('='*50, '\\nCNN\\n', '='*50)\n",
                "cnn = CNN(INPUT_DIM, filt=32, ks=2, drop=0.3, nc=NC)\n",
                "cnn = train_model(cnn, 'CNN', X_train, y_train, X_val, y_val, epochs=20, lr=0.001)\n",
                "cp = {'Train': predict_torch(cnn, tdl), 'Val': predict_torch(cnn, vdl), 'Test': predict_torch(cnn, sdl)}\n",
                "plot_confusion_matrices('CNN', ys, cp, le.classes_, CKPT_DIR)\n",
                "save_ckpt(cnn, 'CNN', CKPT_DIR, pytorch=True, metrics={'test_f1': f1_score(y_test, cp['Test'], average='weighted')})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. Summary + Overfitting Check\n",
                "import pandas as pd\n",
                "\n",
                "all_p = {'KNN': kp, 'SVM': sp, 'RF': rp, 'ANN': ap, 'CNN': cp}\n",
                "rows = {}\n",
                "for m, preds in all_p.items():\n",
                "    for s in ['Train','Val','Test']:\n",
                "        rows[f'{m}_{s}'] = {\n",
                "            'Accuracy': accuracy_score(ys[s], preds[s]),\n",
                "            'F1': f1_score(ys[s], preds[s], average='weighted')}\n",
                "\n",
                "print('\\n'+'='*60+'\\nFULL RESULTS\\n'+'='*60)\n",
                "print(pd.DataFrame(rows).T.to_string(float_format='%.4f'))\n",
                "\n",
                "print('\\n--- Overfitting Check ---')\n",
                "for m in ['KNN','SVM','RF','ANN','CNN']:\n",
                "    tr = rows[f'{m}_Train']['F1']; te = rows[f'{m}_Test']['F1']; g = tr-te\n",
                "    st = 'OK' if g<0.05 else 'OVERFIT' if g>0.1 else 'MODERATE'\n",
                "    print(f'  {m:4s}: Train={tr:.4f} Test={te:.4f} Gap={g:.4f} [{st}]')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 11. List Checkpoints\n",
                "print('Saved checkpoints:')\n",
                "for f in sorted(os.listdir(CKPT_DIR)):\n",
                "    sz = os.path.getsize(os.path.join(CKPT_DIR, f)) / (1024*1024)\n",
                "    print(f'  {f} ({sz:.1f} MB)')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}